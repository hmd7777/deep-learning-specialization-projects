# Optimization and Regularization Techniques âš™ï¸

This project explores different strategies to improve neural-network performance and stability.

## ğŸ§© Concepts Demonstrated
- Weight initialization (He, Xavier)
- L2 regularization and dropout
- Mini-batch gradient descent
- Adam optimizer implementation
- Learning-rate scheduling and decay
- Gradient checking

## âš™ï¸ Tools
Python 3.11, NumPy, Matplotlib

## ğŸ“Š Result
Improved convergence speed and reduced overfitting versus baseline.

## ğŸ“š Reference
Assignment from *DeepLearning.AI â€“ Improving Deep Neural Networks (Coursera)*.
