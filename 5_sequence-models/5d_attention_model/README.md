# Attention Model for Machine Translation âœ¨

Implements the **Bahdanau (additive) attention** mechanism to enhance sequence-to-sequence translation performance.

## ğŸ§© Concepts Demonstrated
- Attention weights and alignment
- Context vector computation
- Visualization of attention heat-maps
- Comparison of vanilla vs attention-based seq2seq models

## âš™ï¸ Tools
Python 3.11, TensorFlow 2/Keras, NumPy, Matplotlib

## ğŸ“Š Result
Significant improvement in translation fluency (â†‘ BLEU score).

## ğŸ“š Reference
Assignment from *DeepLearning.AI â€“ Sequence Models (Coursera)*.
